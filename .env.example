# Environment variables for LLM providers
NODE_ENV=development

# Choose your current LLM provider: openai, anthropic, gemini, ollama
LLM_PROVIDER=openai

# Toggle LLM functionality (set to 'false' to disable LLM features)
ENABLE_LLM=true

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_DEFAULT_MODEL=gpt-4o-mini

# Anthropic Configuration  
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Gemini Configuration
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_DEFAULT_MODEL=gemini-2.5-flash

# Ollama Configuration (for local models)
OLLAMA_BASE_URL=http://localhost:11434

# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=konkani_dictionary
DB_USER=konkani_dev
DB_PASSWORD=dev_password_2024

# Server Configuration
PORT=3001