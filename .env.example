# Environment variables for LLM providers
NODE_ENV=development

# Choose your current LLM provider: openai, anthropic, ollama
LLM_PROVIDER=openai

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Configuration  
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Ollama Configuration (for local models)
OLLAMA_BASE_URL=http://localhost:11434

# Server Configuration
PORT=3001

# Database Configuration (PostgreSQL - Railway)
DB_HOST=postgres.railway.internal
DB_PORT=5432
DB_NAME=konkani_dictionary
DB_USER=postgres
DB_PASSWORD=your_database_password_here

# Dictionary API URL (Railway deployment)
DICTIONARY_API_URL=https://konkani-dictionary-production.up.railway.app/api/dictionary

# LLM Toggle - set to 'false' to disable LLM-based responses
ENABLE_LLM=true